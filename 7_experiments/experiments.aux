\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experiments}{29}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:experiments}{{5}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Tokenization of the dataset}{29}{}\protected@file@percent }
\newlabel{chapter:data_chunking}{{5.1}{29}}
\@writefile{lol}{\contentsline {listing}{\numberline {5}{\ignorespaces Creating the Dataset Dictionary\relax }}{29}{}\protected@file@percent }
\newlabel{code:dict}{{5}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Format of Dataset Dictionary\relax }}{29}{}\protected@file@percent }
\newlabel{fig:dict_features}{{5.1}{29}}
\@writefile{lol}{\contentsline {listing}{\numberline {6}{\ignorespaces Tokenize Function to call on the text\relax }}{29}{}\protected@file@percent }
\newlabel{code:tok_funct}{{6}{29}}
\@writefile{lol}{\contentsline {listing}{\numberline {7}{\ignorespaces Applying the tokenize function on the Text\relax }}{30}{}\protected@file@percent }
\newlabel{code:tok_map}{{7}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Text collumn has been replaced by calling tokenizer on the dataset.\relax }}{30}{}\protected@file@percent }
\newlabel{fig:dict_tokenized}{{5.2}{30}}
\@writefile{lol}{\contentsline {listing}{\numberline {8}{\ignorespaces Definition of the method group\_text\relax }}{30}{}\protected@file@percent }
\newlabel{code:group_text}{{8}{30}}
\citation{trainer}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Training of the Tokenizer}{31}{}\protected@file@percent }
\newlabel{sec:tokenizer}{{5.2}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Extending the tokenizers vocabulary}{31}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {9}{\ignorespaces Extending the Tokenizer Vocabulary\relax }}{31}{}\protected@file@percent }
\newlabel{code:extend_tokenizer}{{9}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Pre-Training the tokenizer}{31}{}\protected@file@percent }
\newlabel{chapter:training-tokenizer}{{5.2.2}{31}}
\newlabel{tok_data}{{5.2.2}{32}}
\@writefile{lol}{\contentsline {listing}{\numberline {10}{\ignorespaces Training the Tokenizer and resize Vocabulary\relax }}{32}{}\protected@file@percent }
\newlabel{code:train_tokenizer}{{10}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Training Process}{32}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {11}{\ignorespaces Creating the Training Arguments\relax }}{32}{}\protected@file@percent }
\newlabel{code:train_args}{{11}{32}}
\@writefile{lol}{\contentsline {listing}{\numberline {12}{\ignorespaces Creating the Trainer Object\relax }}{33}{}\protected@file@percent }
\newlabel{code:trainer}{{12}{33}}
\@writefile{lol}{\contentsline {listing}{\numberline {13}{\ignorespaces Starting the training\relax }}{33}{}\protected@file@percent }
\newlabel{code:train}{{13}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Moving the bias towards FA domain}{33}{}\protected@file@percent }
\newlabel{chapter:training-experiments}{{5.3.1}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Loss Function during Training with Infineon Dataset and small trained tokenizer.\relax }}{34}{}\protected@file@percent }
\newlabel{fig:loss_small}{{5.3}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Loss Function during Training with Infineon Dataset and large trained tokenizer.\relax }}{34}{}\protected@file@percent }
\newlabel{fig:loss_large}{{5.4}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Loss Function during Training with Infineon Dataset and extended tokenizer.\relax }}{35}{}\protected@file@percent }
\newlabel{fig:loss_ext}{{5.5}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Loss Function during Training with tokenizer trained on s2.\relax }}{35}{}\protected@file@percent }
\newlabel{fig:loss_s2}{{5.6}{35}}
\citation{loss}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Loss Function during Training with largest dataset and tokenizer trained on large.\relax }}{36}{}\protected@file@percent }
\newlabel{fig:loss_large_large}{{5.7}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Trying different types of losses}{36}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {14}{\ignorespaces Trainer compute loss function\relax }}{36}{}\protected@file@percent }
\newlabel{code:loss}{{14}{36}}
\@writefile{lol}{\contentsline {listing}{\numberline {15}{\ignorespaces Cross Entropy Loss\relax }}{37}{}\protected@file@percent }
\newlabel{code:cel}{{15}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Changing the Masking Method}{37}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Loss Function during Training with Infineon Dataset and large trained tokenizer.\relax }}{37}{}\protected@file@percent }
\newlabel{fig:data_wordids}{{5.8}{37}}
\@writefile{lol}{\contentsline {listing}{\numberline {16}{\ignorespaces Data Collator implementing Whole Word Masking\relax }}{38}{}\protected@file@percent }
\newlabel{code:wwm}{{16}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Example of Whole Word Masking Method.\relax }}{38}{}\protected@file@percent }
\newlabel{fig:wwm_ex}{{5.9}{38}}
\@writefile{lol}{\contentsline {listing}{\numberline {17}{\ignorespaces Trainer\relax }}{38}{}\protected@file@percent }
\newlabel{code:trainer_wwm}{{17}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Loss Function during Training with Whole Word Masking.\relax }}{39}{}\protected@file@percent }
\newlabel{fig:loss_wwm}{{5.10}{39}}
\@setckpt{7_experiments/experiments}{
\setcounter{page}{40}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{float@type}{16}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{FancyVerbLine}{7}
\setcounter{linenumber}{1}
\setcounter{LN@truepage}{49}
\setcounter{FV@TrueTabGroupLevel}{0}
\setcounter{FV@TrueTabCounter}{0}
\setcounter{FV@HighlightLinesStart}{0}
\setcounter{FV@HighlightLinesStop}{0}
\setcounter{FancyVerbLineBreakLast}{0}
\setcounter{minted@FancyVerbLineTemp}{30}
\setcounter{minted@pygmentizecounter}{18}
\setcounter{listing}{17}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{lstlisting}{0}
}
