\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{5}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:literaturereview}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Natural Language Processing}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Text Representation}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Tokenization}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Bag of Words}{6}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Example of scoring methods\relax }}{7}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:scoring}{{2.1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Term Frequency - Inverse Document Frequency}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Word Embeddings}{7}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces One Hot Encoding Example\relax }}{8}{}\protected@file@percent }
\newlabel{tab:onehot}{{2.2}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec Architecture}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Masked Language Modelling}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Transformer}{8}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Transformer Architecture\relax }}{9}{}\protected@file@percent }
\newlabel{fig:transformer}{{2.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Evolution of Language Models}{9}{}\protected@file@percent }
\citation{Devlin}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Timeline of Model evolution in NLP.\relax }}{10}{}\protected@file@percent }
\newlabel{fig:timeline}{{2.2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}BERT}{10}{}\protected@file@percent }
\newlabel{bert}{{2.3.3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Embeddings of Text Input with BERT.\relax }}{11}{}\protected@file@percent }
\newlabel{fig:bert_tokenizing}{{2.3}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Pre-Training and Fine-Tuning Process of BERT\relax }}{11}{}\protected@file@percent }
\newlabel{fig:bert_tokenizing}{{2.4}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Difference from BERT and word2Vec}{11}{}\protected@file@percent }
\@setckpt{3_literaturereview/literaturereview}{
\setcounter{page}{13}
\setcounter{equation}{3}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{2}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{float@type}{16}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{linenumber}{1}
\setcounter{LN@truepage}{22}
\setcounter{FV@TrueTabGroupLevel}{0}
\setcounter{FV@TrueTabCounter}{0}
\setcounter{FV@HighlightLinesStart}{0}
\setcounter{FV@HighlightLinesStop}{0}
\setcounter{FancyVerbLineBreakLast}{0}
\setcounter{minted@FancyVerbLineTemp}{0}
\setcounter{minted@pygmentizecounter}{0}
\setcounter{listing}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{lstlisting}{0}
}
