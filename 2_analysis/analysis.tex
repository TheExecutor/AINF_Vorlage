\chapter{Analysis} \label{chapter:analysis}
Introduction to chapter


All of the following steps to prepare the dataset for training the model were taken from an online \alert{notebook} created by \textit{Google}. 

\section{Training Environment}

\section{Training Pipeline}

\subsection{Finding the Model Entrypoint}
Since BERT models studied in this paper were trained on different text corpora, their tokenizers might provide different coverage of important terms used in the FA domain, such as the ones stored in FA ontology. To determine the coverage, the first task was to analyze the tokenization of the ontology keywords with the three language models: traditional BERT, SciBERT and S2ORC-SciBERT. With this analysis it was possible to gain a first impression about the vocabulary of the models and how well they fit to the electrical domain. \newline
To receive the keywords, we executed an existing function provided from our supervisor Christian Burmer. It executes a script with a request to the SparQL Database receiving the ontology keywords in a list. This keywords also occur in the reports as indicators for the content. \newline
After receiving the keywords from the ontology and having a deeper look at them, we found some misspellings like in the word \textit{flasover}, which misses a h to become \textit{flashover}. These typos could confuse the models and lead to some mistokenization. To prevent this, we will first run some spell checking and correction methods on the keywords before further processing it. Therefore we tried different packages like the \textit{Pyspellchecker} or \textit{SymSpellPy} which are based on the \textit{Peter Novig's method}. As both of the two packages did not work, we skimmed the keywords manually and corrected misspellings. \newline
With the correct spelled keywords we started the experiment of tokenizing them with the three different BERT models. The code for tokenization was the same for every model. We created a loop which hands every word over to the tokenizers and also analyzes if the resulting word is included in the models vocabulary or not. If it is not included in the vocabulary, we counted the number of good and badly tokenized words

\alert{\subsection{S2ORC-SciBERT Fine-Tuning}}
Also the tokenizer itself can be improved before using it in the training method together with the model. Training a to-kenizer can be done in two ways:

\begin{enumerate}
	\item Extending the vocabulary manually
	\item Pre-training the tokenizer 
\end{enumerate}

Tokenizers of BERT-derived models usually have empty positions allowing us to add required tokens directly. Extend-ing the vocabulary of a tokenizer means that it wonâ€™t get trained explicitely but important words like our ontology keywords can be added to the vocabulary of the tokenizer and therefore recognized as one single token by the model later. Pre-training the tokenizer means that the tokenizer will get trained on a prepared dataset without using the model. Again the engineer has no impact on the behavior of the tokenizer during the training loops. 
We created four types of trained tokenizers. For later discus-sions we introduce the following abbreviations:

\begin{itemize}
	\item \textbf{ext}: A tokenizer with the ontology keywords added to its vocabulary manually
	\item \textbf{small}: A tokenizer trained on inf dataset
	\item \textbf{large}: A tokenizer trained on inf+s2 dataset;
	\item \textbf{flt}: A tokenizer trained on the s2 dataset
\end{itemize}



Tokenizing dataset
split into blocks
MLM Method
different models 


