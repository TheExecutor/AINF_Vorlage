\begin{thebibliography}{MeVO96}

\bibitem{Khurana}
Khurana, Diksha, u. a. „Natural Language Processing: State of the Art, Current Trends and Challenges“. Multimedia Tools and Applications, Juli 2022. Springer Link, https://doi.org/10.1007/s11042-022-13428-4.

\bibitem{onehot}
One-Hot Encoding - an overview | ScienceDirect Topics. https://www.sciencedirect.com/topics/computer-science/one-hot-encoding. Zugegriffen 4. Jänner 2023.

\bibitem{Sammut}
Sammut, Claude, und Geoffrey I. Webb, Herausgeber. „TF–IDF“. Encyclopedia of Machine Learning, Springer US, 2010, S. 986–87. Springer Link, https://doi.org/10.1007/978-0-387-30164-8_832.

\bibitem{word2vec}
15.1. Word Embedding (word2vec) — Dive into Deep Learning 1.0.0-beta0 documentation. https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html. Zugegriffen 4. Jänner 2023.

\bibitem{Vaswani}
Vaswani, Ashish, u. a. Attention Is All You Need. arXiv, 5. Dezember 2017. arXiv.org, https://doi.org/10.48550/arXiv.1706.03762.
  
\bibitem{Devlin}
Devlin, Jacob, u. a. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv, 24. Mai 2019. arXiv.org, https://doi.org/10.48550/arXiv.1810.04805.

\bibitem{Liu}
Liu, Yinhan, u. a. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv, 26. Juli 2019. arXiv.org, https://doi.org/10.48550/arXiv.1907.11692.

\bibitem{Rasmi}
Rasmy, Laila, u. a. Med-BERT: pre-trained contextualized embeddings on large-scale structured electronic health records for disease prediction. arXiv, 22. Mai 2020. arXiv.org, https://doi.org/10.48550/arXiv.2005.12833.

\bibitem{Beltagy}
Beltagy, Iz, u. a. SciBERT: A Pretrained Language Model for Scientific Text. arXiv, 10. September 2019. arXiv.org, https://doi.org/10.48550/arXiv.1903.10676.

\bibitem{Lo}
Lo, Kyle, u. a. S2ORC: The Semantic Scholar Open Research Corpus. arXiv, 6. Juli 2020. arXiv.org, https://doi.org/10.48550/arXiv.1911.02782.

\bibitem{models}
Models - Hugging Face. https://huggingface.co/models. Zugegriffen 4. Jänner 2023.

\bibitem{nltk}
NLTK :: Natural Language Toolkit. https://www.nltk.org/. Zugegriffen 4. Jänner 2023.

\bibitem{transformers}
Transformers. https://huggingface.co/docs/transformers/index. Zugegriffen 4. Jänner 2023.

\bibitem{trainer}
Trainer. https://huggingface.co/transformers/v4.8.0/main_classes/main_classes/trainer.html. Zugegriffen 4. Jänner 2023.

\bibitem{loss}
Wolf, Thomas, u. a. Transformers: State-of-the-Art Natural Language Processing. 2018. Association for Computational Linguistics, Oktober 2020. GitHub, https://www.aclweb.org/anthology/2020.emnlp-demos.6.

\end{thebibliography}
