\contentsline {chapter}{\numberline {1}Introduction}{1}{}%
\contentsline {section}{\numberline {1.1}Problem Definition}{1}{}%
\contentsline {subsection}{\numberline {1.1.1}Research Questions}{2}{}%
\contentsline {chapter}{\numberline {2}Literature Review}{3}{}%
\contentsline {section}{\numberline {2.1}Natural Language Processing}{3}{}%
\contentsline {subsection}{\numberline {2.1.1}Tokenization}{3}{}%
\contentsline {subsection}{\numberline {2.1.2}Evolution of Language Models}{4}{}%
\contentsline {subsection}{\numberline {2.1.3}Bag of Words}{4}{}%
\contentsline {subsection}{\numberline {2.1.4}Term Frequency - Inverse Document Frequency}{5}{}%
\contentsline {subsection}{\numberline {2.1.5}Word Embeddings}{6}{}%
\contentsline {subsubsection}{Word2Vec Architecture}{6}{}%
\contentsline {subsection}{\numberline {2.1.6}Transformer}{7}{}%
\contentsline {subsection}{\numberline {2.1.7}BERT}{7}{}%
\contentsline {subsubsection}{Difference from BERT and word2Vec}{9}{}%
\contentsline {subsection}{\numberline {2.1.8}From BERT to SciBERT}{9}{}%
\contentsline {section}{\numberline {2.2}Masked Language Modelling}{9}{}%
\contentsline {section}{\numberline {2.3}Named Entity Recognition}{9}{}%
\contentsline {chapter}{\numberline {3}Analysis}{11}{}%
\contentsline {section}{\numberline {3.1}BERT based Language Models}{11}{}%
\contentsline {section}{\numberline {3.2}Finding the Model Entrypoint}{11}{}%
\contentsline {chapter}{\numberline {4}Data Collection}{15}{}%
\contentsline {section}{\numberline {4.1}Data Sources}{15}{}%
\contentsline {subsection}{\numberline {4.1.1}Failure Analysis Ontology and Reports}{15}{}%
\contentsline {subsection}{\numberline {4.1.2}S2ORC Dataset}{16}{}%
\contentsline {subsection}{\numberline {4.1.3}Infineon Dataset}{17}{}%
\contentsline {subsection}{\numberline {4.1.4}Additional Data}{17}{}%
\contentsline {section}{\numberline {4.2}Data Preprocessing}{17}{}%
\contentsline {subsection}{\numberline {4.2.1}Data Cleaning}{18}{}%
\contentsline {subsection}{\numberline {4.2.2}Dataset Formatting}{19}{}%
\contentsline {chapter}{\numberline {5}Experiments}{21}{}%
\contentsline {section}{\numberline {5.1}Tokenization of the dataset}{21}{}%
\contentsline {section}{\numberline {5.2}Training of the Tokenizer}{23}{}%
\contentsline {subsection}{\numberline {5.2.1}Extending the tokenizers vocabulary}{23}{}%
\contentsline {subsection}{\numberline {5.2.2}Pre-Training the tokenizer}{23}{}%
\contentsline {section}{\numberline {5.3}Training Process}{24}{}%
\contentsline {subsection}{\numberline {5.3.1}Moving the bias towards FA domain}{25}{}%
\contentsline {subsection}{\numberline {5.3.2}Trying different types of losses}{27}{}%
\contentsline {subsection}{\numberline {5.3.3}Changing the Masking Method}{29}{}%
\contentsline {chapter}{\numberline {6}Results}{33}{}%
\contentsline {section}{\numberline {6.1}Performance Measures}{33}{}%
\contentsline {section}{\numberline {6.2}Discussion of Results}{33}{}%
\contentsline {chapter}{\numberline {7}Conclusion}{35}{}%
\contentsline {section}{\numberline {7.1}Future Work}{35}{}%
\contentsline {chapter}{Bibliography}{37}{}%
